# ğŸ’° **Cost Function in Machine Learning**

The **cost function** measures how well our **hypothesis function** $h_\theta(x)$ predicts the actual outcomes $y$. It's a way to quantify the **error** between predicted and actual values.

---

### ğŸ§® **Cost Function Formula (for Linear Regression)**

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x_i) - y_i \right)^2
$$

Where:

* $J(\theta_0, \theta_1)$: The **cost** (or error) associated with parameters $\theta_0$ and $\theta_1$
* $m$: Number of training examples
* $h_\theta(x_i)$: Predicted value for input $x_i$
* $y_i$: Actual value

---

### ğŸ§  **What This Means:**

* It calculates the **average of the squared differences** between predicted and actual values.
* The **squared error** penalizes larger deviations more heavily.
* The factor $\frac{1}{2}$ is for mathematical convenience â€” it cancels out during differentiation (e.g., for gradient descent).

---

### ğŸ§¾ Interpretation:

* **Low cost** â†’ predictions are close to actual values.
* **High cost** â†’ large prediction errors; the model isn't accurate.

---

### ğŸ“Œ Also Known As:

* **Squared Error Function**
* **Mean Squared Error (MSE)**

---

### ğŸ” Used In:

* **Linear Regression**
* **Gradient Descent** optimization to minimize the cost

---

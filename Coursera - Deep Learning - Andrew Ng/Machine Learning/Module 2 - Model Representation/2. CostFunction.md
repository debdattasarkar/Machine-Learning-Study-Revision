# 💰 **Cost Function in Machine Learning**

The **cost function** measures how well our **hypothesis function** $h_\theta(x)$ predicts the actual outcomes $y$. It's a way to quantify the **error** between predicted and actual values.

---

### 🧮 **Cost Function Formula (for Linear Regression)**

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x_i) - y_i \right)^2
$$

Where:

* $J(\theta_0, \theta_1)$: The **cost** (or error) associated with parameters $\theta_0$ and $\theta_1$
* $m$: Number of training examples
* $h_\theta(x_i)$: Predicted value for input $x_i$
* $y_i$: Actual value

---

### 🧠 **What This Means:**

* It calculates the **average of the squared differences** between predicted and actual values.
* The **squared error** penalizes larger deviations more heavily.
* The factor $\frac{1}{2}$ is for mathematical convenience — it cancels out during differentiation (e.g., for gradient descent).

---

### 🧾 Interpretation:

* **Low cost** → predictions are close to actual values.
* **High cost** → large prediction errors; the model isn't accurate.

---

### 📌 Also Known As:

* **Squared Error Function**
* **Mean Squared Error (MSE)**

---

### 🔁 Used In:

* **Linear Regression**
* **Gradient Descent** optimization to minimize the cost

---

# üí∞ **Cost Function in Machine Learning**

The **cost function** measures how well our **hypothesis function** $h_\theta(x)$ predicts the actual outcomes $y$. It's a way to quantify the **error** between predicted and actual values.

---

### üßÆ **Cost Function Formula (for Linear Regression)**

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x_i) - y_i \right)^2
$$

Where:

* $J(\theta_0, \theta_1)$: The **cost** (or error) associated with parameters $\theta_0$ and $\theta_1$
* $m$: Number of training examples
* $h_\theta(x_i)$: Predicted value for input $x_i$
* $y_i$: Actual value

---

### üß† **What This Means:**

* It calculates the **average of the squared differences** between predicted and actual values.
* The **squared error** penalizes larger deviations more heavily.
* The factor $\frac{1}{2}$ is for mathematical convenience ‚Äî it cancels out during differentiation (e.g., for gradient descent).

---

### üßæ Interpretation:

* **Low cost** ‚Üí predictions are close to actual values.
* **High cost** ‚Üí large prediction errors; the model isn't accurate.

---

### üìå Also Known As:

* **Squared Error Function**
* **Mean Squared Error (MSE)**

---

### üîÅ Used In:

* **Linear Regression**
* **Gradient Descent** optimization to minimize the cost

---

![CostFunction - Coursera](images/CostFunction.png)

This image is a visual explanation of how the **cost function** works in **linear regression**, and how it is used to find the best-fit line by adjusting the parameters $\theta_0$ and $\theta_1$.

---

### üß† **Core Idea:**

> Choose $\theta_0, \theta_1$ so that $h_\theta(x)$ is **close to $y$** for all training examples $(x, y)$.

---

### üìä Left Side (Graph):

* **Red crosses** = Training data points $(x^{(i)}, y^{(i)})$
* **Line** = Hypothesis $h_\theta(x) = \theta_0 + \theta_1 x$
* The goal is to adjust $\theta_0, \theta_1$ so that the line fits the data well (i.e., minimizes prediction error).

---

### üßÆ Right Side (Math):

The cost function:

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
$$

* Measures the **average squared error** between the predictions and actual values.
* It‚Äôs also called the **squared error function**.
* The objective is to **minimize** this cost:

  $$
  \min_{\theta_0, \theta_1} J(\theta_0, \theta_1)
  $$

---

### üßæ Key Concepts Highlighted:

| Symbol                  | Meaning                                      |
| ----------------------- | -------------------------------------------- |
| $h_\theta(x)$           | Hypothesis function (prediction)             |
| $\theta_0, \theta_1$    | Parameters to be learned                     |
| $J(\theta_0, \theta_1)$ | Cost function (error metric)                 |
| $x^{(i)}, y^{(i)}$      | A single training example‚Äôs input and output |
| $m$                     | Number of training examples                  |

---

### ‚úÖ Summary:

* This diagram summarizes how **linear regression** learns by minimizing a **cost function**.
* The **goal of learning** is to find parameters that reduce the **prediction error** on training data.


The **cost function** used in linear regression ‚Äî specifically, the **Mean Squared Error (MSE)** function with a factor of $\frac{1}{2m}$, which is used to simplify derivative calculations during gradient descent.

---

### üìå **Formula Explained:**

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
$$

Where:

* $J(\theta_0, \theta_1)$: Cost function ‚Äî measures how well the model with parameters $\theta_0, \theta_1$ fits the data.
* $m$: Number of training examples
* $\hat{y}_i$: Predicted value from the model for example $i$
* $y_i$: Actual (true) value for example $i$

---

### üí° **Purpose:**

This cost function computes the **average squared difference** between predicted and actual values:

* Small value of $J$ ‚Üí predictions are close to actual values (good fit)
* Large value of $J$ ‚Üí poor model fit (needs better parameters)

---

### üß† **Why the $\frac{1}{2}$ factor?**

* It simplifies the **derivative** during gradient descent because the square term‚Äôs derivative brings down a 2, which cancels with the $\frac{1}{2}$.

---

The formula for the **cost function** $J(\theta_0, \theta_1)$ used in **linear regression**.

---

### üßÆ Full Cost Function (Mean Squared Error):

![CostFunction - Coursera](images/CostFunction2.png)
---

### üîç Components Explained:

| Symbol                  | Meaning                                                   |
| ----------------------- | --------------------------------------------------------- |
| $J(\theta_0, \theta_1)$ | Cost function (error measure) for linear regression       |
| $m$                     | Number of training examples                               |
| $\hat{y}_i$             | Predicted output for the $i$-th input                     |
| $y_i$                   | Actual output for the $i$-th input                        |
| $h_\theta(x_i)$         | Hypothesis function, defined as $\theta_0 + \theta_1 x_i$ |

---

### üß† Purpose:

This function tells us **how well our model predicts** the actual values.

* If predictions match real values ‚Üí $J(\theta_0, \theta_1)$ is **small**
* If predictions are far off ‚Üí the value is **large**

---

### ‚úÖ Use in Training:

We try to **minimize** $J(\theta_0, \theta_1)$ using algorithms like **gradient descent**, so that our model learns the best possible parameters.

### üß† **Question Breakdown & Solution**

You are given:

* A training set with $m = 3$ data points
* Hypothesis: $h_\theta(x) = \theta_1 x$
* Cost function:

$$
J(\theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

* You are asked: **What is $J(0)$?**

---

### üìä **From the Plot: Training Examples**

![Graph - Coursera](images/graph.jpg)

Extract data points from the chart:

* $(x=0, y=1)$
* $(x=1, y=2)$
* $(x=2, y=3)$

---

### üßÆ **Step-by-Step Calculation for $\theta_1 = 0$**

Since $h_\theta(x) = \theta_1 x$, if $\theta_1 = 0$:

$$
h_\theta(x) = 0 \quad \text{for all } x
$$

Compute the cost:

$$
J(0) = \frac{1}{2 \cdot 3} \left[(0 - 1)^2 + (0 - 2)^2 + (0 - 3)^2\right]
= \frac{1}{6} \left[1 + 4 + 9\right]
= \frac{14}{6}
$$

‚úÖ This matches the **correct answer**: **$\boxed{14/6}$**

---

### üîÅ Summary

| $x$ | $y$ | $h_\theta(x)$ | Error $(h_\theta(x) - y)^2$ |
| --- | --- | ------------- | --------------------------- |
| 0   | 1   | 0             | 1                           |
| 1   | 2   | 0             | 4                           |
| 2   | 3   | 0             | 9                           |
|     |     |               | **Total = 14**              |
|     |     |               | $J(0) = \frac{14}{6}$       |

Let me know if you'd like a Markdown version or graph walkthrough added to documentation!
